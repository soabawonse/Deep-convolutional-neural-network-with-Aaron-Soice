from core import *
from torch_backend import *
import argparse

def conv_bn(c_in, c_out, bn_weight_init=1.0, **kw):
    return {
        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), 
        'bn': batch_norm(c_out, bn_weight_init=bn_weight_init, **kw), 
        'relu': nn.ReLU(True)
    }

def residual(c, **kw):
    return {
        'in': Identity(),
        'res1': conv_bn(c, c, **kw),
        'res2': conv_bn(c, c, **kw),
        'add': (Add(), [rel_path('in'), rel_path('res2', 'relu')]),
    }

def basic_net(channels, weight,  pool, **kw):
    return {
        'prep': conv_bn(3, channels['prep'], **kw),
        'layer1': dict(conv_bn(channels['prep'], channels['layer1'], **kw), pool=pool),
        'layer2': dict(conv_bn(channels['layer1'], channels['layer2'], **kw), pool=pool),
        'layer3': dict(conv_bn(channels['layer2'], channels['layer3'], **kw), pool=pool),

        'pool': nn.MaxPool2d(4),
        'flatten': Flatten(),
        'linear': nn.Linear(channels['layer3'], 10, bias=False),
        'classifier': Mul(weight),
    }

def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3'), **kw):
    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}
    n = basic_net(channels, weight, pool, **kw)
    for layer in res_layers:
        n[layer]['residual'] = residual(channels[layer], **kw)
    for layer in extra_layers:
        n[layer]['extra'] = conv_bn(channels[layer], channels[layer], **kw)       
    return n

def main():

  parser = argparse.ArgumentParser(
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)

  parser.add_argument("--batch_size",
                      help="Batch Size",
                      type=int,
                      default=512)

  parser.add_argument("--num_runs",
                      help="Number of runs",
                      type=int,
                      default=5)

  parser.add_argument("--device_ids",
                      help="list of GPU devices",
                      type=str,
                      default="0")  

  params = parser.parse_args()

  losses = {
      'loss':  (nn.CrossEntropyLoss(reduce=False), [('classifier',), ('target',)]),
      'correct': (Correct(), [('classifier',), ('target',)]),
  }

  DATA_DIR = './data'
  dataset = cifar10(root=DATA_DIR)
  t = Timer()
  print('Preprocessing training data')
  train_set = list(zip(transpose(normalise(pad(dataset['train']['data'], 4))), dataset['train']['labels']))
  print(f'Finished in {t():.2} seconds')
  print('Preprocessing test data')
  test_set = list(zip(transpose(normalise(dataset['test']['data'])), dataset['test']['labels']))
  print(f'Finished in {t():.2} seconds')

  epochs=24
  lr_schedule = PiecewiseLinear([0, 5, epochs], [0, 0.4, 0])
  batch_size = params.batch_size
  transforms = [Crop(32, 32), FlipLR(), Cutout(8, 8)]
  N_runs = params.num_runs

  device_ids = [int(n) for n in params.device_ids.split(',')]

  train_batches = Batches(Transform(train_set, transforms), batch_size, shuffle=True, set_random_choices=True, drop_last=True)
  test_batches = Batches(test_set, batch_size, shuffle=False, drop_last=False)
  lr = lambda step: lr_schedule(step/len(train_batches))/batch_size

  summaries = []
  for i in range(N_runs):
      print(f'Starting Run {i} at {localtime()}')
      # model = Network(union(net(), losses)).to(device).half()

      model = nn.DataParallel(Network(union(net(), losses)).half(), device_ids=device_ids)
      model.to(device)

      opt = SGD(trainable_params(model), lr=lr, momentum=0.9, weight_decay=5e-4*batch_size, nesterov=True)
      summaries.append(train(model, opt, train_batches, test_batches, epochs, loggers=(TableLogger(),)))

  test_accs = np.array([s['test acc'] for s in summaries])
  print(f'mean test accuracy: {np.mean(test_accs):.4f}')
  print(f'median test accuracy: {np.median(test_accs):.4f}')
  print(f'{np.sum(test_accs>=0.94)}/{N_runs} >= 94%')


if __name__ == "__main__":
  main()